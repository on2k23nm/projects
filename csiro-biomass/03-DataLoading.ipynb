{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f94f486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T13:46:55.643006Z",
     "iopub.status.busy": "2025-12-07T13:46:55.642426Z",
     "iopub.status.idle": "2025-12-07T13:46:55.646922Z",
     "shell.execute_reply": "2025-12-07T13:46:55.646166Z",
     "shell.execute_reply.started": "2025-12-07T13:46:55.642978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import importlib, csiro_biomass_shared_utils as csiro_su\n",
    "importlib.reload(csiro_su)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Sequence\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # don't split into blocks\n",
    "pd.set_option('display.width', 200)               # or None for auto / very wide\n",
    "pd.set_option('display.max_columns', None)        # show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff78c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 50 51\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_split_dfs(in_dir=\"dataset/\"):\n",
    "    in_dir = Path(in_dir)\n",
    "\n",
    "    train_df = pd.read_parquet(in_dir / \"train.parquet\")\n",
    "    val_df   = pd.read_parquet(in_dir / \"val.parquet\")\n",
    "    test_df  = pd.read_parquet(in_dir / \"test.parquet\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# usage\n",
    "train_df, val_df, test_df = load_split_dfs(\"dataset/\")\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "\n",
    "cols = [\"Pre_GSHH_NDVI\",\"Height_Ave_cm\",\"Dry_Clover_g\",\"Dry_Dead_g\",\"Dry_Green_g\",\"Dry_Total_g\",\"GDM_g\"]\n",
    "\n",
    "csiro_su._assert_has_columns(train_df, cols, \"train_df\")\n",
    "csiro_su._assert_has_columns(val_df, cols, \"val_df\")\n",
    "csiro_su._assert_has_columns(test_df, cols, \"test_df\")\n",
    "\n",
    "# print(train_df)\n",
    "# print(\"All good\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad446e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfms_train = build_image_transforms(\"train\", target_h=256, target_w=512)\n",
    "# # csiro_su.show_original_vs_transformed(train_df, \"/kaggle/input/csiro-biomass/train\", tfms_train, n=3, font_size=9)\n",
    "\n",
    "# def collate_with_original(batch):\n",
    "#     images = torch.stack([b[\"image\"] for b in batch], dim=0)\n",
    "#     ids = [b[\"sample_id\"] for b in batch]\n",
    "\n",
    "#     out = {\"image\": images, \"sample_id\": ids}\n",
    "\n",
    "#     if \"orig\" in batch[0]:\n",
    "#         out[\"orig\"] = [b[\"orig\"] for b in batch]  # list of [3,H,W]\n",
    "\n",
    "#     return out\n",
    "\n",
    "# ds = Image2BiomassData(train_df, \"/kaggle/input/csiro-biomass/train\",\n",
    "#                        mode=\"train\", return_original=True)\n",
    "\n",
    "# dl = DataLoader(ds, batch_size=8, shuffle=True, num_workers=2,\n",
    "#                 collate_fn=collate_with_original)\n",
    "\n",
    "# csiro_su.show_dl_batch_original_vs_transformed(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362da68",
   "metadata": {},
   "source": [
    "## Model to predict from the image:\n",
    "\n",
    "* **Biomass (regression):** `Dry_Clover_g, Dry_Dead_g, Dry_Green_g, Dry_Total_g, GDM_g`\n",
    "* **Aux targets to “boost” backbone (multi-task):**\n",
    "\n",
    "  * `Pre_GSHH_NDVI` (regression)\n",
    "  * `Height_Ave_cm` (regression)\n",
    "  * `Species` (**classification**, not regression)\n",
    "\n",
    "And **everything else** can be returned as **metadata** (for debugging / weighting / stratification), but not trained as targets.\n",
    "\n",
    "auxiliary tasks (NDVI/Height/Species) can help the backbone learn better visual features, which often improves the main biomass R².\n",
    "\n",
    "#### About \"other columns as metadata to boost R²\"\n",
    "\n",
    "Yes — keep them in `meta` for:\n",
    "\n",
    "* tail weighting (`TailFlag`)\n",
    "* regime-aware sampling (`Cbin/Tbin/...`)\n",
    "* analysis/debug plots\n",
    "* sanity checks\n",
    "\n",
    "…but **don’t train to predict them** (it’s usually just noise/overhead).\n",
    "\n",
    "#### Dataset that returns (image → multi-target) + metadata\n",
    "\n",
    "* **Image**\n",
    "\n",
    "  * `out[\"image\"] = img_t` is the **transformed** image tensor (resize/augment/normalize).\n",
    "  * `out[\"orig\"] = orig` is included **only if** `self.return_original=True` (and it’s the *original* image tensor you saved for visualization/debug).\n",
    "\n",
    "* **Targets / labels**\n",
    "\n",
    "  * `out[\"y_biomass\"]` = the 5 biomass targets: `[\"Dry_Clover_g\",\"Dry_Dead_g\",\"Dry_Green_g\",\"Dry_Total_g\",\"GDM_g\"]` (possibly log/standardized depending on how you created `y_biomass`)\n",
    "  * `out[\"y_ndvi\"]`, `out[\"y_height\"]`, `out[\"y_species\"]` = whatever you computed (raw or transformed/encoded).\n",
    "\n",
    "* **IDs / metadata**\n",
    "\n",
    "  * `out[\"sample_id\"]` = identifier\n",
    "  * `out[\"meta\"] = row.to_dict()` is included **only if** `self.return_meta=True`. It contains the full row (all columns), so yes it can carry “auxiliary” info — **but it’s not automatically used to boost R²** unless your model forward/loss actually consumes those meta fields.\n",
    "\n",
    "So: **the dataset returns everything you need**, but “meta boosts R²” happens only if you explicitly feed some/all of it into the model (or use it for sampling/weights).\n",
    "\n",
    "Clean way to do this - \n",
    "* **Dataset = “raw fetcher”** (reads image + row/meta, no transforms)\n",
    "* **One “batch preprocessor” function = does *all* transforms** (image + targets + keeps `meta`, `orig`, `sample_id`)\n",
    "* Plug that preprocessor into the **DataLoader via `collate_fn`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2f50d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "# ---------- small helpers ----------\n",
    "def _to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_tail_q_from_train_df(train_df: pd.DataFrame, target_cfg: dict, stats: dict, qs=(0.90, 0.99), eps: float = 1e-8):\n",
    "    tail_q = {}\n",
    "\n",
    "    for col, cfg in target_cfg.items():\n",
    "        s = pd.to_numeric(train_df[col], errors=\"coerce\")  # Series with NaNs possible\n",
    "\n",
    "        # 1) log1p (if configured)\n",
    "        if cfg.get(\"log1p\", False):\n",
    "            s = np.log1p(np.clip(s.values.astype(np.float32), 0.0, None))\n",
    "        else:\n",
    "            s = s.values.astype(np.float32)\n",
    "\n",
    "        # 2) normalize (if configured)\n",
    "        if cfg.get(\"normalize\", True):\n",
    "            mu, sigma = stats[col]\n",
    "            sigma = float(sigma) if float(sigma) > eps else eps\n",
    "            s = (s - float(mu)) / sigma\n",
    "\n",
    "        # 3) quantiles (ignore NaNs)\n",
    "        qvals = np.nanquantile(s, qs)\n",
    "\n",
    "        tail_q[col] = {f\"q{int(q*100)}\": float(v) for q, v in zip(qs, qvals)}\n",
    "\n",
    "    return tail_q\n",
    "\n",
    "\n",
    "def col_mean_std_from_series(s: pd.Series, ddof: int = 0, eps: float = 1e-8):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        raise ValueError(\"No valid numeric values to compute mean/std.\")\n",
    "    mu = float(s.mean())\n",
    "    sigma = float(s.std(ddof=ddof))\n",
    "    if (not np.isfinite(sigma)) or sigma < eps:\n",
    "        sigma = eps\n",
    "    return mu, sigma\n",
    "\n",
    "class Image2BiomassData(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, images_dir: str,\n",
    "                 mode: str = \"train\", export_state: bool = False,\n",
    "                 import_state: bool = False, state: dict | None = None):\n",
    "\n",
    "        self.df                 = dataframe.reset_index(drop=True).copy()\n",
    "        self.images_dir         = images_dir\n",
    "        self.mode               = mode\n",
    "        self.stats              = {}\n",
    "        \n",
    "        # fixed columns (no extra init args)\n",
    "        # sample_id Species  Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g GDM_g\n",
    "        # SpeciesBucket Tbin Gbin Dbin Cbin Rbin  GroupID  TailFlag  StratifyKey StratifyKey_safe\n",
    "        self.id_col         = \"sample_id\"\n",
    "        self.species_col    = \"Species\"\n",
    "        self.numeric_cols   = [\"Pre_GSHH_NDVI\", \"Height_Ave_cm\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "        self.meta_cols      = [\"SpeciesBucket\", \"Tbin\", \"Gbin\", \"Dbin\", \"Cbin\", \"Rbin\", \"GroupID\", \"TailFlag\", \"StratifyKey\", \"StratifyKey_safe\"]\n",
    "     \n",
    "        # Decide per-column transform (example)\n",
    "        self.target_cfg = {\n",
    "            \"Pre_GSHH_NDVI\": {\"log1p\": False, \"normalize\": True},\n",
    "            \"Height_Ave_cm\": {\"log1p\": True,  \"normalize\": True},\n",
    "            \"Dry_Clover_g\":  {\"log1p\": True,  \"normalize\": True},\n",
    "            \"Dry_Dead_g\":    {\"log1p\": True,  \"normalize\": True},\n",
    "            \"Dry_Green_g\":   {\"log1p\": True,  \"normalize\": True},\n",
    "            \"Dry_Total_g\":   {\"log1p\": True,  \"normalize\": True},\n",
    "            \"GDM_g\":         {\"log1p\": True,  \"normalize\": True},\n",
    "        }\n",
    "        \n",
    "        # ---- IMPORT STATE (no refit) ----\n",
    "        if import_state:\n",
    "            if state is None:\n",
    "                raise ValueError(\"import_state=True requires a non-None `state`.\")\n",
    "            self.stats      = state[\"stats\"]\n",
    "            self.species2id = state[\"species2id\"]\n",
    "            self.target_cfg = state.get(\"target_cfg\", self.target_cfg)\n",
    "            self.tail_q      = state.get(\"tail_q\", None)\n",
    "        else:\n",
    "            # ---- FIT ON THIS DATAFRAME (train) ----\n",
    "            # species mapping (fit from provided dataframe)\n",
    "            uniq = sorted(self.df[self.species_col].astype(str).unique().tolist())\n",
    "            self.species2id = {\"__UNK__\": 0, **{s: i + 1 for i, s in enumerate(uniq)}}\n",
    "            \n",
    "            # 1) fit stats (needed for normalization)\n",
    "            for col, cfg in self.target_cfg.items():\n",
    "                s = pd.to_numeric(self.df[col], errors=\"coerce\")\n",
    "                if cfg.get(\"log1p\", False):\n",
    "                    # log1p requires x >= -1; for biomass it should be >= 0\n",
    "                    s = np.log1p(np.clip(s, 0, None))\n",
    "                mu, sigma = col_mean_std_from_series(s)\n",
    "                self.stats[col] = (mu, sigma)\n",
    "            \n",
    "            # 2) compute tail quantiles on transformed+standardized scale\n",
    "            self.tail_q = compute_tail_q_from_train_df(self.df, self.target_cfg, self.stats,\n",
    "                                                       qs=[0.50, 0.75, 0.90, 0.95, 0.99])\n",
    "            print(self.tail_q)\n",
    "        \n",
    "        # ---- optionally store exportable state ----\n",
    "        self._state = self.get_state() if export_state else None\n",
    "        \n",
    "        IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "        IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "        # optional torch versions (handy for unnormalize)\n",
    "        IMAGENET_MEAN_T = torch.tensor(IMAGENET_MEAN).view(3,1,1)\n",
    "        IMAGENET_STD_T  = torch.tensor(IMAGENET_STD).view(3,1,1)\n",
    "\n",
    "        # build image tfms ONCE (do not recreate per sample)\n",
    "        TARGET_H, TARGET_W = 256, 512\n",
    "        if self.mode == \"train\":\n",
    "            self.img_tfms = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Resize((TARGET_H, TARGET_W), interpolation=InterpolationMode.BICUBIC),\n",
    "\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomVerticalFlip(p=0.5),\n",
    "\n",
    "                T.RandomApply([T.RandomRotation(\n",
    "                    degrees=5,\n",
    "                    interpolation=InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )], p=0.25),\n",
    "\n",
    "                T.RandomApply([T.ColorJitter(\n",
    "                    brightness=0.2, contrast=0.2, saturation=0.15, hue=0.03\n",
    "                )], p=0.25),\n",
    "\n",
    "                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ])\n",
    "        else:\n",
    "            self.img_tfms = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Resize((TARGET_H, TARGET_W), interpolation=InterpolationMode.BICUBIC),\n",
    "                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ])\n",
    "\n",
    "    def get_state(self) -> dict:\n",
    "        return {\n",
    "            \"stats\": self.stats,\n",
    "            \"species2id\": self.species2id,\n",
    "            \"target_cfg\": self.target_cfg,\n",
    "            \"tail_q\": self.tail_q\n",
    "        }\n",
    "            \n",
    "    def _transform_and_standardize(self, col: str, x: float) -> float:\n",
    "        cfg = self.target_cfg[col]\n",
    "\n",
    "        # 1) transform\n",
    "        if cfg.get(\"log1p\", False):\n",
    "            x = np.log1p(max(x, 0.0))\n",
    "\n",
    "        # 2) optional standardization\n",
    "        if cfg.get(\"normalize\", True):\n",
    "            mu, sigma = self.stats[col]\n",
    "            x = (x - mu) / sigma\n",
    "\n",
    "        return float(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _img_path(self, sid: str) -> Path:\n",
    "        return f\"{self.images_dir}/{sid}.jpg\"\n",
    "\n",
    "    def _transform_image(self, img_path: Path) -> torch.Tensor:\n",
    "        img_bgr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"Image not found or unreadable: {img_path}\")\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return self.img_tfms(img_rgb)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        ## -- Get the right index ---\n",
    "        row = self.df.iloc[idx]\n",
    "        sid = str(row[self.id_col])\n",
    "        \n",
    "        img_t = self._transform_image(f\"{self.images_dir}/{sid}.jpg\")\n",
    "        \n",
    "        out = {\"img_t\": img_t}\n",
    "        \n",
    "        ## -- sample_id : nothing changes --\n",
    "        out.update({ \"sample_id\": sid})\n",
    "       \n",
    "        # species -> id (use precomputed mapping)\n",
    "        sp = str(row[self.species_col])\n",
    "        sp_id = self.species2id.get(sp, self.species2id[\"__UNK__\"])\n",
    "        out[\"species_id\"] = torch.tensor(sp_id, dtype=torch.long)\n",
    "        \n",
    "        ## All the remaining columns:\n",
    "        # Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g GDM_g\n",
    "        for c in self.target_cfg.keys():\n",
    "            out[c] = torch.tensor(self._transform_and_standardize(c, _to_float(row[c])), dtype=torch.float32)\n",
    "                \n",
    "        # copy remaining/meta columns AS-IS (no transforms)\n",
    "        for k in self.meta_cols:\n",
    "            if k in row.index:\n",
    "                out[k] = row[k]\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ac7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pre_GSHH_NDVI': {'q50': 0.21079139411449432, 'q75': 0.7403659224510193, 'q90': 1.2037436962127686, 'q95': 1.3361376523971558, 'q99': 1.4632354521751414}, 'Height_Ave_cm': {'q50': -0.2719688415527344, 'q75': 0.3856850862503052, 'q90': 1.4404025077819824, 'q95': 2.3810505628585807, 'q99': 2.7710587978363037}, 'Dry_Clover_g': {'q50': -0.2647244334220886, 'q75': 0.7284159064292908, 'q90': 1.5074969053268432, 'q95': 1.8269424676895119, 'q99': 2.515016226768495}, 'Dry_Dead_g': {'q50': 0.05714616924524307, 'q75': 0.7739342451095581, 'q90': 1.264405083656311, 'q95': 1.4368308544158934, 'q99': 1.7236804413795472}, 'Dry_Green_g': {'q50': 0.21901564300060272, 'q75': 0.6587365567684174, 'q90': 1.1036961793899536, 'q95': 1.3031319260597227, 'q99': 1.667626738548279}, 'Dry_Total_g': {'q50': 0.10779093205928802, 'q75': 0.6682067811489105, 'q90': 1.2109348773956299, 'q95': 1.5225960612297054, 'q99': 1.9445340704917924}, 'GDM_g': {'q50': 0.1054128035902977, 'q75': 0.6984666883945465, 'q90': 1.186800456047058, 'q95': 1.4667258858680716, 'q99': 1.951317517757416}}\n"
     ]
    }
   ],
   "source": [
    "data_root_dir = \"/kaggle/input/csiro-biomass\"\n",
    "\n",
    "train_ds = Image2BiomassData(train_df, f\"{data_root_dir}/train\", mode=\"train\", export_state=True)\n",
    "state = train_ds.get_state()\n",
    "# print(state)\n",
    "\n",
    "val_ds = Image2BiomassData(val_df, f\"{data_root_dir}/train\", mode=\"val\", import_state=True, state=state)\n",
    "state = val_ds.get_state()\n",
    "# print(state)\n",
    "\n",
    "test_ds = Image2BiomassData(test_df, f\"{data_root_dir}/train\", mode=\"val\", import_state=True, state=state)\n",
    "state = val_ds.get_state()\n",
    "# print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "758ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "      \n",
    "TARGET_COLS = [\"Pre_GSHH_NDVI\", \"Height_Ave_cm\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "def collate_biomass(batch):\n",
    "    out = {}\n",
    "    out[\"img_t\"] = torch.stack([b[\"img_t\"] for b in batch], dim=0)              # [B,3,256,512]\n",
    "    out[\"sample_id\"] = [b[\"sample_id\"] for b in batch]                          # list[str]\n",
    "    out[\"species_id\"] = torch.stack([b[\"species_id\"] for b in batch], dim=0)    # [B]\n",
    "    \n",
    "    # targets -> [B, 8]\n",
    "    out[\"y\"] = torch.stack(\n",
    "        [torch.stack([b[c] for c in TARGET_COLS]) for b in batch], dim=0\n",
    "    ).float()\n",
    "\n",
    "    # keep meta as-is (strings/ints) -> list\n",
    "    for k in batch[0].keys():\n",
    "        if k in [\"img_t\", \"species_id\", \"sample_id\"] + TARGET_COLS:\n",
    "            continue\n",
    "        out[k] = [b[k] for b in batch]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a998e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True, collate_fn=collate_biomass)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_biomass)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_biomass)\n",
    "\n",
    "# b = next(iter(train_dl))\n",
    "# print(\"img_t:\", b[\"img_t\"].shape, b[\"img_t\"].dtype)\n",
    "# print(\"y:\", b[\"y\"].shape, b[\"y\"].dtype)\n",
    "# print(\"species_id:\", b[\"species_id\"].shape, b[\"species_id\"].dtype)\n",
    "# print(\"sample_id:\", len(b[\"sample_id\"]), b[\"sample_id\"][0])\n",
    "# print(\"Training & Validation dataloaders created !\")\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016aaa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv-dl-rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
